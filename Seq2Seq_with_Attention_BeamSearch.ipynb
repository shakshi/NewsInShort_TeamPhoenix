{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attn+Beam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "05XZQDsQxrpt",
        "colab_type": "code",
        "outputId": "f1cf8e10-93ea-4131-8245-58ef9cb90078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mounting google drive \n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjfo4iPp0YOP",
        "colab_type": "code",
        "outputId": "832fc14a-d87b-4d56-996d-55188b366348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!pip install glove-python\n",
        "# importing libraries\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import sys\n",
        "import re            \n",
        "from glove import Glove"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove-python) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove-python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700268 sha256=b8fe7f23b78c9abdc018a94190f4a72c08650ea8abb4fc595696cb89509c4ae0\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHPH4ocS_OiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append('/content/drive/My Drive/NLP- News summary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJSaDmEJ_gCI",
        "colab_type": "code",
        "outputId": "a2cfb9e9-7041-4e27-c129-ea6914eecbf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from attention import AttentionLayer\n",
        "import warnings\n",
        "\n",
        "from keras import backend as K \n",
        "from keras.initializers import Constant "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37NHM0e2x51X",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Reading Data** \n",
        "\n",
        "Read processed articles and summaries generated by DataProcessing.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXV5iXUSxxJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "fp= open('/content/drive/My Drive/article_highlight_Sample.json', 'r')\n",
        "content= fp.read()\n",
        "result=json.loads(content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOCQ3NDZj5Nf",
        "colab_type": "code",
        "outputId": "975ba991-d4ad-4641-ea03-24076cdf93eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#data after preprocessing -\n",
        "print(\"After cleaning - \")\n",
        "print(\"Article: \", result[str(1)][\"article\"])\n",
        "print(\"Summary:\", result[str(1)][\"highlight\"])\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After cleaning - \n",
            "Article:  associated press published 14 11 est 25 october 2013 updated 15 36 est 25 october 2013 bishop fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis virus late september early october state health department issued advisory exposure anyone attended five churches took communion bishop john folda pictured fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis state immunization program manager molly howell says risk low officials feel important alert people possible exposure diocese announced monday bishop john folda taking time diagnosed hepatitis diocese says contracted infection contaminated food attending conference newly ordained bishops italy last month symptoms hepatitis include fever tiredness loss appetite nausea abdominal discomfort fargo catholic diocese north dakota pictured bishop located\n",
            "Summary:  bishop john folda  of north dakota  is taking time off after being diagnosed   he contracted the infection through contaminated food in italy   church members in fargo  grand forks and jamestown could have been exposed   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlsGeq9NlHc0",
        "colab_type": "text"
      },
      "source": [
        "**Data** **Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjF-1BL6ybBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating dataframe\n",
        "articles=[]\n",
        "summaries=[]\n",
        "d={}\n",
        "\n",
        "for i, obj in result.items():\n",
        "  articles.append(obj[\"article\"])\n",
        "  summaries.append(obj[\"highlight\"])\n",
        "\n",
        "d[\"article\"]= articles\n",
        "d[\"summary\"]= summaries\n",
        "\n",
        "df= pd.DataFrame.from_dict(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-APZPbMzJr3",
        "colab_type": "code",
        "outputId": "37d0a181-1210-468f-c05f-65690a915758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>associated press published 14 11 est 25 octobe...</td>\n",
              "      <td>bishop john folda  of north dakota  is taking...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ralph mata internal affairs lieutenant miami d...</td>\n",
              "      <td>criminal complaint  cop used his role to help...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>drunk driver killed young woman head crash che...</td>\n",
              "      <td>craig eccleston todd  27  had drunk at least ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>breezy sweep pen president vladimir putin wrot...</td>\n",
              "      <td>nina dos santos says europe must be ready to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>leetwood team still 100 record sky bet league ...</td>\n",
              "      <td>fleetwood top of league one after 2 0 win at ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             article                                            summary\n",
              "0  associated press published 14 11 est 25 octobe...   bishop john folda  of north dakota  is taking...\n",
              "1  ralph mata internal affairs lieutenant miami d...   criminal complaint  cop used his role to help...\n",
              "2  drunk driver killed young woman head crash che...   craig eccleston todd  27  had drunk at least ...\n",
              "3  breezy sweep pen president vladimir putin wrot...   nina dos santos says europe must be ready to ...\n",
              "4  leetwood team still 100 record sky bet league ...   fleetwood top of league one after 2 0 win at ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PnG8eLx_e9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : \"_START_ \"+ x + \" _END_\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_xBxnxVzdmM",
        "colab_type": "text"
      },
      "source": [
        "**Counting numbers of words in articles and summaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzvekGkSzLVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# word count\n",
        "for article in df['article']:\n",
        "      article_word_count.append(len(article.split()))\n",
        "\n",
        "for summary in df['summary']:\n",
        "      summary_word_count.append(len(summary.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRZrFhYiz2zA",
        "colab_type": "code",
        "outputId": "4ca60a97-403a-4151-ecd5-ecd46f5783bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "length_df = pd.DataFrame({'article':article_word_count, 'summary':summary_word_count})\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBwo2u340J_R",
        "colab_type": "code",
        "outputId": "f7ff88a3-5003-46d9-e142-64197ca01dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "lengths_articles = pd.DataFrame(article_word_count, columns=['counts'])\n",
        "lengths_summaries = pd.DataFrame(summary_word_count, columns=['counts'])\n",
        "\n",
        "print(\"Articles:\")\n",
        "print(lengths_articles.describe())\n",
        "\n",
        "print(\"\\nSummaries:\")\n",
        "print(lengths_summaries.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Articles:\n",
            "             counts\n",
            "count  60000.000000\n",
            "mean     393.090717\n",
            "std      187.290512\n",
            "min       13.000000\n",
            "25%      256.000000\n",
            "50%      359.000000\n",
            "75%      495.000000\n",
            "max     2000.000000\n",
            "\n",
            "Summaries:\n",
            "             counts\n",
            "count  60000.000000\n",
            "mean      51.357567\n",
            "std       20.961405\n",
            "min        8.000000\n",
            "25%       38.000000\n",
            "50%       48.000000\n",
            "75%       59.000000\n",
            "max     1247.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MRNLWLvBzBt",
        "colab_type": "code",
        "outputId": "050e9543-7394-4cfb-fc11-0bbfd4ef44f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Inspect the length of articles\n",
        "print(np.percentile(lengths_articles.counts, 90))\n",
        "print(np.percentile(lengths_articles.counts, 95))\n",
        "print(np.percentile(lengths_articles.counts, 99))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "653.0\n",
            "763.0\n",
            "945.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjyOdlgHDIu9",
        "colab_type": "code",
        "outputId": "5f213a1b-09bf-4976-e7b3-f762db1d43fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Inspect the length of summaries\n",
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "76.0\n",
            "88.0\n",
            "117.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTUunmNiC2aR",
        "colab_type": "text"
      },
      "source": [
        "~90 percentile of the articles contain less than 653 words and ~90 percentile of summaries contain less than 74 words\n",
        "\n",
        "We use this data to set a reasonable value for **max_len_text** and **max_len_summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eNVt3XbC0PP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#max_len_text= 500\n",
        "max_len_text= 300\n",
        "max_len_summary= 70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BtbcrknE0x7",
        "colab_type": "text"
      },
      "source": [
        "**Word Embeddings**\n",
        "\n",
        "We use pre-trained word embeddings from GLoVe \n",
        "\n",
        "These word embeddings were generated by training model on Wikipedia 2014 and Gigaword Dataset \n",
        "\n",
        "Vocabulary Size: 400,000 words \n",
        "\n",
        "Embedding vector dimension: 50\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrLfxd-1DVQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove= Glove()\n",
        "model= glove.load_stanford('/content/drive/My Drive/NLP- News summary/gloveEng/glove.6B.300d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xdfjxIJtvQ3t"
      },
      "source": [
        "Printing sample word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-1ZnUSWFxKP",
        "colab_type": "code",
        "outputId": "b9dfd5d3-404e-4b41-d284-0281f533fae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.word_vectors[model.dictionary['woman']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.51821  , -0.13809  , -0.41185  , -0.13133  ,  0.0035659,\n",
              "       -0.31205  , -0.31242  , -0.43538  , -0.27017  , -1.1338   ,\n",
              "        0.19282  , -0.23786  ,  0.0028892, -0.027078 ,  0.14747  ,\n",
              "       -0.051265 ,  0.089021 , -0.12337  , -0.40892  , -0.39197  ,\n",
              "       -0.5665   ,  0.40684  , -0.057313 ,  0.18613  , -0.33095  ,\n",
              "       -0.25243  ,  0.33452  , -0.23104  , -0.0099149,  0.24269  ,\n",
              "       -0.57504  ,  0.30122  , -0.33779  , -0.294    , -0.80202  ,\n",
              "        0.44674  , -0.40296  , -0.21026  ,  0.1072   ,  0.53937  ,\n",
              "        0.63971  , -0.3154   , -0.082537 , -0.038314 , -0.058821 ,\n",
              "        0.11235  ,  0.50928  ,  0.14656  , -0.18988  ,  0.24132  ,\n",
              "       -0.038442 ,  0.071418 ,  0.38471  ,  0.10584  , -0.52786  ,\n",
              "       -0.057574 ,  0.13771  , -0.48613  ,  0.62553  , -0.3308   ,\n",
              "       -0.065539 ,  0.1791   ,  0.62746  ,  0.087208 , -0.60422  ,\n",
              "       -0.64595  , -0.2389   , -0.1585   , -0.07948  , -0.037848 ,\n",
              "       -0.205    , -0.44504  , -0.21127  , -0.0026664, -0.0080152,\n",
              "        0.39607  ,  0.069091 , -0.34379  , -0.13992  ,  0.084029 ,\n",
              "       -0.40245  ,  0.094426 ,  0.28908  ,  0.6216   , -0.4132   ,\n",
              "       -0.12153  , -0.40676  , -0.05771  ,  0.17415  ,  0.094069 ,\n",
              "       -0.65955  , -0.34154  , -0.079291 ,  0.16867  ,  0.31665  ,\n",
              "       -0.072868 , -0.44181  , -0.19555  ,  0.16942  , -0.197    ,\n",
              "        0.17484  ,  0.11972  ,  0.012957 , -0.32238  ,  0.33737  ,\n",
              "        0.35553  ,  0.82906  , -0.37521  ,  0.34777  , -0.13105  ,\n",
              "       -0.046688 ,  1.2125   ,  0.025435 , -0.36021  ,  0.65259  ,\n",
              "        0.63729  ,  0.23457  ,  0.19874  ,  0.22589  , -0.016769 ,\n",
              "        0.10671  ,  0.86805  , -0.02853  , -0.011634 , -0.39911  ,\n",
              "        0.12429  , -0.066363 ,  0.0080237,  0.41826  , -0.60603  ,\n",
              "        0.26269  ,  0.035625 , -0.10225  ,  0.44362  , -0.38952  ,\n",
              "       -0.054241 , -0.31542  , -0.076452 , -0.33072  ,  0.42178  ,\n",
              "        0.29242  ,  0.13222  , -0.14896  ,  0.32047  , -0.4735   ,\n",
              "       -0.1093   ,  0.31163  ,  0.49836  , -0.20143  ,  0.36058  ,\n",
              "        0.32242  , -0.11148  ,  0.6416   ,  0.20955  ,  0.035183 ,\n",
              "        0.16684  , -0.41427  , -0.41894  ,  0.18777  ,  0.39424  ,\n",
              "        0.25784  ,  0.18818  ,  0.60567  ,  0.15537  , -0.27121  ,\n",
              "        0.054047 , -0.18342  ,  0.29789  ,  0.35805  , -0.40148  ,\n",
              "       -0.019914 , -0.019742 , -0.56609  , -0.25878  , -0.036075 ,\n",
              "        0.0093725, -0.25284  , -0.061715 , -0.26441  ,  0.47597  ,\n",
              "        0.087956 ,  0.051997 ,  0.21366  , -0.0034455,  0.1739   ,\n",
              "       -0.16853  , -0.22233  , -0.1006   , -0.032696 , -0.008549 ,\n",
              "        0.036532 , -0.19339  , -0.28571  , -0.29294  , -0.53655  ,\n",
              "        0.16387  , -0.36861  , -0.52443  , -0.84287  ,  0.26247  ,\n",
              "        1.8261   ,  0.029467 ,  0.19155  ,  0.28406  , -0.1017   ,\n",
              "       -0.31416  , -0.084328 ,  0.42934  ,  0.32851  ,  0.41274  ,\n",
              "       -0.080323 ,  0.063666 , -0.18441  ,  0.13328  ,  0.46     ,\n",
              "       -0.24984  ,  0.12574  , -0.49056  , -0.072603 ,  0.28191  ,\n",
              "       -0.25738  ,  0.40629  ,  0.38381  , -0.37685  , -0.16371  ,\n",
              "        0.30354  , -0.38234  , -0.61633  , -0.22076  ,  0.38153  ,\n",
              "        0.54091  , -0.32349  , -0.032075 , -0.051326 , -0.12465  ,\n",
              "        0.19237  , -0.077144 ,  0.27005  , -0.20103  , -0.26512  ,\n",
              "        0.35769  , -0.23437  ,  0.054273 ,  0.16901  , -0.15758  ,\n",
              "        0.42714  ,  0.23167  , -0.021318 ,  0.3086   ,  0.44873  ,\n",
              "       -0.12432  , -0.15715  , -0.099448 , -0.24825  ,  1.156    ,\n",
              "       -0.38925  , -0.0063171,  0.48928  ,  0.46089  , -0.17058  ,\n",
              "        0.06118  , -0.54     , -0.054482 ,  0.13329  , -0.47944  ,\n",
              "        0.17119  ,  0.26289  ,  0.14383  , -0.30443  ,  0.27534  ,\n",
              "       -0.14711  , -0.52172  ,  0.32909  , -0.15149  ,  0.1539   ,\n",
              "        0.24171  , -1.6971   ,  0.027579 , -0.0073776,  0.30144  ,\n",
              "        0.011751 ,  0.012419 ,  0.38711  , -0.044167 , -0.62495  ,\n",
              "        0.74536  ,  0.043054 ,  0.62925  , -0.33381  , -0.048651 ,\n",
              "        0.09395  , -0.20336  , -0.055232 ,  0.096572 ,  0.09321  ,\n",
              "       -0.17298  , -0.20794  ,  0.37342  , -0.030166 ,  0.73014  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytj57gjKFx5e",
        "colab_type": "text"
      },
      "source": [
        "Printing words with similar word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-9uyUw0Fl_H",
        "colab_type": "code",
        "outputId": "237eb292-7c58-486c-a2ce-d058b344a0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(model.most_similar('woman'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('girl', 0.7296419112178933), ('man', 0.6998663379619018), ('mother', 0.6899437807604597), ('she', 0.6433226707219909)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul5tomJTF7OU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating embedding dictionary\n",
        "embeddings_index = {}\n",
        "\n",
        "for word in model.dictionary:\n",
        "  embedding = model.word_vectors[model.dictionary[word]]\n",
        "  embeddings_index[word] = embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9RDilvsTG8M",
        "colab_type": "code",
        "outputId": "b38bc24c-82aa-4b51-968c-da5d1247007c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(embeddings_index[\"the\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsCll6PHIKdf",
        "colab_type": "text"
      },
      "source": [
        "**Split Data for training and testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHQMc3hXITWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr,x_test,y_tr,y_test = train_test_split(df['article'],df['summary'],test_size=0.1,random_state=0,shuffle=True) \n",
        "\n",
        "x_tr,x_val,y_tr,y_val = train_test_split(x_tr,y_tr,test_size=0.1,random_state=0,shuffle=True) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqdTIfa3IfO-",
        "colab_type": "code",
        "outputId": "90a45ef5-d617-4a4e-920d-1115665136e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Training data: ', len(x_tr))\n",
        "print('Validation data: ', len(x_val))\n",
        "print('Testing data:', len(x_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data:  48600\n",
            "Validation data:  5400\n",
            "Testing data: 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNRaE1uMG4Ta",
        "colab_type": "text"
      },
      "source": [
        "**Tokenize Data, create vocabulary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6SBJcSeFEbI",
        "colab_type": "text"
      },
      "source": [
        "Article Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMzGnpCWGTqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "before_sample=x_tr[0]\n",
        "# Tokenizer library in keras that maps words to integers, integers to words\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "x_voc_size   =  len(x_tokenizer.word_index) +1\n",
        "\n",
        "# convert text sequences into integer sequences\n",
        "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "# padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
        "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
        "after_sample=x_tr[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJwKzOyPXzsu",
        "colab_type": "code",
        "outputId": "485ca6c5-b795-4ef6-c086-3b69b3993b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "print(\"Aricle Before: \\n\"+str(before_sample[0:250]))\n",
        "print()\n",
        "print(\"Article After: \\n\"+str(after_sample[0:250]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aricle Before: \n",
            "associated press published 14 11 est 25 october 2013 updated 15 36 est 25 october 2013 bishop fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis virus late september early october sta\n",
            "\n",
            "Article After: \n",
            "[  617 14815  9358     8   157   127   674    21 59656  1767   610   343\n",
            "  6428    85   544  2298    35    85   714  4080  2853    33  4066  4075\n",
            "  3104  1210   544 11953   351   545   923   333   747   323  8314  2298\n",
            "  2177   716    33   686  1850  1716 12674    11   127  1716  4762  8660\n",
            "   360  4762  3543 10459    18  3171  4045     7  5198     9   161   166\n",
            "   102  2298    96  3787  3096   107   161  4080   521    35   320 16913\n",
            "     8  1767  1502  1210    10  2504    14    52     8  4676  4484    69\n",
            " 10266  1716    32  1767    52  4374  4465   213   233   667   207   544\n",
            "  9934   556  1002    10   806     2  9053  5219   288   483  1130   939\n",
            "  4033  9934  4965   570   220   127    14   987  1092   815 21057   938\n",
            "  1236   190   914  1437 23148   555   987   768  6301   358    14   606\n",
            "   570   189     8   157  9934   122  2103  2077  8563  2637  1762     1\n",
            "     4    59 30681   873   895    97   544  1435  9934   584  4408   426\n",
            " 11048 18163   461   544  4972  1792  9252   461   164  1454  1035  3007\n",
            " 56541  3579  6039     1  1559  7506 12520  4134  7009  2970  2794  7963\n",
            "   225   322 28792    10  1775     2 12520  8493 17115   119   159  6039\n",
            "     4    18   106 10820    56  6720  6039   461   975 16914  1822   895\n",
            "     1  2364   815  7506  1941    53   333   435    53   725  1097    53\n",
            "    87   333   751   270 44151    58     7  6515  2086  4555  9328    12\n",
            "  9934   584   267  4429   533    14   190 90116  2139  1452     2  4429\n",
            "   520 21434 21434   451 15124  6378   461   112 12628  6163]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_PAIkNFFJcL",
        "colab_type": "text"
      },
      "source": [
        "Summary Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JnZ-NWpIw55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing a tokenizer for summary on training data \n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "y_voc_size  =   len(y_tokenizer.word_index) +1\n",
        "\n",
        "# same for summaries\n",
        "y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
        "y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ-NgI0FXqOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35pJdzPBJPEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create embedding matrix to be used in model\n",
        "embedding_dim= 300\n",
        "\n",
        "x_embedding_matrix= np.zeros((x_voc_size, embedding_dim))\n",
        "\n",
        "for word, i in x_tokenizer.word_index.items():\n",
        "  if i> x_voc_size:\n",
        "    continue\n",
        "  embedding_vector= embeddings_index.get(word)\n",
        "\n",
        "  if embedding_vector is not None:\n",
        "    x_embedding_matrix[i]= embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7o-zYmt8fgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding matrix for decoder to be used in model\n",
        "y_embedding_matrix= np.zeros((y_voc_size, embedding_dim))\n",
        "\n",
        "for word, i in y_tokenizer.word_index.items():\n",
        "  if i> y_voc_size:\n",
        "    continue\n",
        "  embedding_vector= embeddings_index.get(word)\n",
        "\n",
        "  if embedding_vector is not None:\n",
        "    y_embedding_matrix[i]= embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDWOd3xKKtCH",
        "colab_type": "code",
        "outputId": "16fec685-f11a-4d78-cd85-729170a6138e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x_embedding_matrix.shape)\n",
        "print(y_embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(202657, 300)\n",
            "(70622, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBuOp5O5Jv9U",
        "colab_type": "text"
      },
      "source": [
        "**Build Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjsgqP2ELAW0",
        "colab_type": "code",
        "outputId": "55063fb6-b7b5-4c3c-8550-8ad6bd22a5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "latent_dim = 300 \n",
        "\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_len_text,)) \n",
        "enc_emb = Embedding(x_voc_size, latent_dim,embeddings_initializer= Constant(x_embedding_matrix), trainable=False)(encoder_inputs) \n",
        "\n",
        "#LSTM 1 \n",
        "encoder_lstm1 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True))\n",
        "encoder_output1, forward_h, forward_c, backward_h, backward_c = encoder_lstm1(enc_emb)\n",
        "\"\"\"\n",
        "state_h1 = Concatenate()([forward_h, backward_h])\n",
        "state_c1 = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h1, state_c1]\n",
        "\"\"\"\n",
        "#LSTM 2 \n",
        "# encoder output 1 is fed as input to the next lstm\n",
        "encoder_lstm2 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True))\n",
        "encoder_output2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_lstm2(encoder_output1)\n",
        "\"\"\"\n",
        "state_h2 = Concatenate()([forward_h2, backward_h2])\n",
        "state_c2 = Concatenate()([forward_c2, backward_c2])\n",
        "encoder_states = [state_h2, state_c2]\n",
        "#encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "\"\"\"\n",
        "#LSTM 3 \n",
        "# encoder output 2 fed as input to next lstm \n",
        "encoder_lstm3=Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True))\n",
        "encoder_output3, forward_h3, forward_c3, backward_h3, backward_c3 = encoder_lstm3(encoder_output2)\n",
        "\"\"\"\n",
        "state_h3 = Concatenate()([forward_h3, backward_h3])\n",
        "state_c3 = Concatenate()([forward_c3, backward_c3])\n",
        "encoder_states3 = [state_h3, state_c3]\n",
        "#encoder_output3, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "\"\"\"\n",
        "#LSTM 4\n",
        "encoder_lstm4=Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True))\n",
        "encoder_outputs, forward_h4, forward_c4, backward_h4, backward_c4 = encoder_lstm2(encoder_output3)\n",
        "state_h = Concatenate()([forward_h4, backward_h4])\n",
        "state_c = Concatenate()([forward_c4, backward_c4])\n",
        "encoder_states = [state_h, state_c]\n",
        "print(state_h.shape)\n",
        "print(state_c.shape)\n",
        "#encoder_outputs, state_h, state_c= encoder_lstm4(encoder_output3) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 600)\n",
            "(None, 600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do2w6u4s8y3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the decoder. \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer=  Embedding(y_voc_size, latent_dim,embeddings_initializer= Constant(y_embedding_matrix), trainable=False)\n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "#LSTM1\n",
        "# Using encoder_states as decoder's initial state\n",
        "decoder_lstm1 = LSTM(2*latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm1(dec_emb,initial_state=encoder_states) \n",
        "\"\"\"\n",
        "#LSTM2 \n",
        "# Using encoder_states as decoder's initial state\n",
        "decoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_output2,decoder_fwd_state, decoder_back_state = decoder_lstm2(decoder_output1) \n",
        "\n",
        "#LSTM3 \n",
        "# Using encoder_states as decoder's initial state\n",
        "decoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_output3,decoder_fwd_state, decoder_back_state = decoder_lstm3(decoder_output2) \n",
        "\n",
        "#LSTM4\n",
        "# Using encoder_states as decoder's initial state\n",
        "decoder_lstm4 = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm4(decoder_output3) \n",
        "\"\"\"\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBsJUdy1YZDp",
        "colab_type": "code",
        "outputId": "1fb82eb2-5922-4068-f544-16576fa0cb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11295217775347888889\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 7429658399249438326\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 14164412718128240060\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15701401920\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 3149159271149227619\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1fLcgt9MAxM",
        "colab_type": "code",
        "outputId": "ceb35751-67b8-4f2e-abf9-63233de92284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# early stopping \n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 300, 300)     60797100    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 300, 600), ( 1442400     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) [(None, 300, 600), ( 2162400     bidirectional[0][0]              \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) [(None, 300, 600), ( 2162400     bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 300)    21186600    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 600)          0           bidirectional_1[1][1]            \n",
            "                                                                 bidirectional_1[1][3]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 600)          0           bidirectional_1[1][2]            \n",
            "                                                                 bidirectional_1[1][4]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, None, 600),  2162400     embedding_1[0][0]                \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 600),  720600      bidirectional_1[1][0]            \n",
            "                                                                 lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1200)   0           lstm_4[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 70622)  84817022    concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 175,450,922\n",
            "Trainable params: 93,467,222\n",
            "Non-trainable params: 81,983,700\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQL1GZA5Qiw_",
        "colab_type": "code",
        "outputId": "64d1ef62-3867-4b40-8330-d5a3f3fa6dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=15,batch_size=32, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1519/1519 [==============================] - 1383s 911ms/step - loss: 4.6174 - val_loss: 4.3331\n",
            "Epoch 2/15\n",
            "1519/1519 [==============================] - 1379s 908ms/step - loss: 4.2942 - val_loss: 4.1881\n",
            "Epoch 3/15\n",
            "1519/1519 [==============================] - 1378s 907ms/step - loss: 4.1392 - val_loss: 4.1285\n",
            "Epoch 4/15\n",
            "1519/1519 [==============================] - 1378s 907ms/step - loss: 4.0377 - val_loss: 4.0999\n",
            "Epoch 5/15\n",
            "1519/1519 [==============================] - 1378s 907ms/step - loss: 3.9545 - val_loss: 4.0831\n",
            "Epoch 6/15\n",
            "1519/1519 [==============================] - 1375s 906ms/step - loss: 3.8798 - val_loss: 4.0858\n",
            "Epoch 7/15\n",
            " 337/1519 [=====>........................] - ETA: 17:08 - loss: 3.7537"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2zyE5AzM_R7",
        "colab_type": "text"
      },
      "source": [
        "Plotting training and validation error to check over-fitting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XJ9dxGoM7vE",
        "colab_type": "code",
        "outputId": "86d31a61-8ae9-4b7a-c488-24397bf0d308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "from matplotlib import pyplot \n",
        "pyplot.plot(history.history['loss'], label='train') \n",
        "pyplot.plot(history.history['val_loss'], label='val') \n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-76ec5627dabd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H6r6c2I14JC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "#model = load_model('/content/drive/My Drive/my_model3.h5',custom_objects={'AttentionLayer': AttentionLayer})\n",
        "model.save('/content/drive/My Drive/my_model_attn_beam.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "#path = '/content/drive/My Drive/my_model4.h5'\n",
        "#model.save(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2yH4JhmGV8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word \n",
        "reverse_source_word_index=x_tokenizer.index_word \n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUz8-xOVGWQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(reverse_source_word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmV89Pg_wzis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "#2 ideas : threshold value if less, remove and if end token found, remove and store in another list and then merge at the end\n",
        "\n",
        "def beam_search(encoder_model,decoder_model, src_input, k, sequence_max_len):\n",
        "\n",
        "    e_out, e_h, e_c = encoder_model.predict(src_input)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = target_word_index['start']\n",
        "\n",
        "    start = True\n",
        "    \n",
        "    #print(k_beam)\n",
        "    # l : point on target sentence to predict\n",
        "    all_k_beams = [[[],1]]\n",
        "    load_hidden={}\n",
        "    end_sentence = []\n",
        "    for l in range(sequence_max_len):\n",
        "\n",
        "        if(start==True):\n",
        "\n",
        "          start=False\n",
        "          output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "          possible_k = output_tokens[0, -1, :].argsort()[-k:][::-1]\n",
        "          #print(\"Possible k in start: \"+str((possible_k)))\n",
        "          dummy_list = []\n",
        "          begin=True\n",
        "          for cand in all_k_beams:\n",
        "            \n",
        "            for curr_k in possible_k:\n",
        "              #print(\"Cand: \"+str(cand)+\" curr_k: \"+str(curr_k))\n",
        "              #print(cand[0]+[curr_k])\n",
        "              #print(cand[1]+np.log(output_tokens[0, -1, :][curr_k]))\n",
        "              curr = [cand[0]+[curr_k],cand[1]-np.log(output_tokens[0, -1, :][curr_k])]\n",
        "              \n",
        "              dummy_list.append(curr)\n",
        "              if(begin==True):\n",
        "                #target_seq = np.zeros((1,1))\n",
        "                #target_seq[0, 0] = curr_k\n",
        "                load_hidden[curr_k] = [h,c]\n",
        "            begin=False\n",
        "          all_k_beams = dummy_list[:]\n",
        "          #print(all_k_beams)\n",
        "          prev_target_seq = copy.deepcopy(possible_k)\n",
        "          #print(\"load_hidden: \"+str(load_hidden.keys()))\n",
        "        else:\n",
        "          #print(\"l is: \"+str(l))\n",
        "          #print(\"In else, prev target seq: \"+str(prev_target_seq))\n",
        "          for target_seq1 in prev_target_seq:\n",
        "            #print(\"Prev index: \"+str(target_seq1))\n",
        "            target_seq = np.zeros((1,1))\n",
        "            target_seq[0, 0] = target_seq1\n",
        "\n",
        "            output_tokens, h, c = decoder_model.predict([target_seq] + [e_out,load_hidden[target_seq1][0],load_hidden[target_seq1][1]])\n",
        "\n",
        "            possible_k = output_tokens[0, -1, :].argsort()[-k:][::-1]\n",
        "            #print(\"Possible k not in start: \"+str((possible_k)))\n",
        "\n",
        "            dummy_list = []\n",
        "            begin = True\n",
        "            for cand in all_k_beams:\n",
        "              for curr_k in possible_k:\n",
        "                if(cand[0][-1]==target_seq1 and len(cand[0])==l):\n",
        "                  #print(\"Here I am\")\n",
        "                  #print(\"Looking at cand: \"+str(cand[0]))\n",
        "                  ended=False\n",
        "                  curr = [cand[0]+[curr_k],cand[1]-np.log(output_tokens[0, -1, :][curr_k])]\n",
        "                  if(curr_k in reverse_target_word_index):\n",
        "                    sampled_token = reverse_target_word_index[curr_k]\n",
        "                    if(sampled_token=='end'):\n",
        "                      end_sentence.append(curr)\n",
        "                      ended=True\n",
        "                  if(ended==False):\n",
        "                    dummy_list.append(curr)\n",
        "                if(begin==True):\n",
        "                  #target_seq = np.zeros((1,1))\n",
        "                  #target_seq[0, 0] = curr_k\n",
        "                  load_hidden[curr_k] = [h,c]\n",
        "              begin=False\n",
        "            all_k_beams +=dummy_list\n",
        "            #print(all_k_beams)\n",
        "            prev_target_seq = copy.deepcopy(possible_k)\n",
        "        \n",
        "        # top k\n",
        "    #print(end_sentence)\n",
        "    all_k_beams += end_sentence\n",
        "    all_k_beams.sort(key=lambda r:r[1])\n",
        "    k_beam = all_k_beams[-k:]\n",
        "    #k_beam = k_beam[::-1]\n",
        "    return k_beam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ5EFfq7GWeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference Phase \n",
        "\n",
        "\"\"\"\n",
        "After training, the model is tested on new source sequences\n",
        "for which the target sequence is not known \n",
        "setting up inference architecture for it \n",
        "\"\"\"\n",
        "\n",
        "# encoder inference\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# decoder inference\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(2*latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(2*latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_len_text,latent_dim*2))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm1(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9fXLd1jJSpm",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    decoded_sentence = beam_search(encoder_model,decoder_model, input_seq, 3, 50)\n",
        "    #decoded_sentence = search(encoder_model,decoder_model, input_seq, 3, max_len_summary)\n",
        "    #decoded_sentence = search(encoder_model,decoder_model, input_seq, 3, 10)\n",
        "\n",
        "    \"\"\"\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = target_word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        if sampled_token_index in reverse_target_word_index:\n",
        "          sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        else:\n",
        "          sampled_token=\"UNK\"\n",
        "          \n",
        "        if(sampled_token!='end'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_summary-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "    \"\"\"\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k00oOQOQGqSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "        newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i!=0):\n",
        "        newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCyB3qx5b22r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentence(model_summary):\n",
        "  all_Sentence = []\n",
        "  for int_sent in model_summary:\n",
        "    sent = int_sent[0]\n",
        "    decoded_sentence= ''\n",
        "    for sampled_token_index in sent:\n",
        "      if sampled_token_index in reverse_target_word_index:\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "      else:\n",
        "        sampled_token=\"UNK\"\n",
        "            \n",
        "      if(sampled_token!='end'):\n",
        "        decoded_sentence += ' '+sampled_token\n",
        "      else:\n",
        "        print(\"broken\")\n",
        "        break\n",
        "    #print(str(decoded_sentence)+ \" , \"+str(int_sent[1]))\n",
        "    all_Sentence.append([decoded_sentence,int_sent[1]])\n",
        "  return all_Sentence \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCRO64l4BSNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "refrence_summary = seq2summary(y_tr[4])\n",
        "model_summary = decode_sequence(x_tr[4].reshape(1,max_len_text))\n",
        "\n",
        "print(\"Reference summary: \"+str(refrence_summary))\n",
        "\n",
        "all_s=get_sentence(model_summary)\n",
        "for pred_s in all_s:\n",
        "  print(\"Predicted summary: \"+str(pred_s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r957VgvFMS94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install rouge\n",
        "from rouge import Rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK39b5Ym5Hyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Writing results to file\")\n",
        "fp = open(\"/content/drive/My Drive/Model_summaries.txt\",'w')\n",
        "for i in range(5):\n",
        "  print(seq2summary(y_val[i]))\n",
        "  fp.write(\"Article:\" + seq2text(x_val[i]))\n",
        "  originalSummary = seq2summary(y_val[i])\n",
        "  fp.write(\"\\nOriginal summary:\" + originalSummary)\n",
        "  predictedSummaries_int = decode_sequence(x_val[i].reshape(1,max_len_text))\n",
        "  predictedSummaries=get_sentence(predictedSummaries_int)\n",
        "  for predictedSummary in predictedSummaries:\n",
        "    fp.write(\"\\nPredicted summary:\" + predictedSummary[0])\n",
        "    print(predictedSummary[0])\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(predictedSummary[0], originalSummary) \n",
        "    print(scores[0]['rouge-1']['f'])\n",
        "    print(scores[0]['rouge-l']['f'])\n",
        "    fp.write(\"\\n\")\n",
        "fp.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p4xBdoRLhQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}